{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed up your training with a proper input pipeline\n",
    "\n",
    "**DISCLAIMER**: the input pipeline API are a not very straightforward and the TF team has already announced they will be rewritten from scratch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "def _prepare_graph():\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(23)\n",
    "_prepare_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three ways to push actual data into a tensorflow graph:\n",
    "1. ~~load all the data in memory (which works only for small datasets)~~\n",
    "2. feeding data at each step\n",
    "3. reading from files and using Queues  \n",
    "  \n",
    "We will compare these two approaches using the same example we used in section 04 where a classifier emits 1 if the sum of all the input elements is positive, 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, inputs, targets):\n",
    "        self._gs = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int32)\n",
    "\n",
    "        self._inputs = inputs\n",
    "        self._targets = targets\n",
    "        \n",
    "        with tf.variable_scope('Layer1') as scope:\n",
    "            w1 = tf.get_variable('w', shape=[1024, 1024])\n",
    "            b1 = tf.get_variable('b', shape=[1024])\n",
    "            z1 = tf.matmul(self._inputs, w1) + b1\n",
    "            y1 = tf.nn.relu(z1, name='activation')\n",
    "\n",
    "        with tf.variable_scope('Layer2') as scope:\n",
    "            w2 = tf.get_variable('w', shape=[1024, 1])\n",
    "            b2 = tf.get_variable('b', shape=[1])\n",
    "            logits = tf.matmul(y1, w2) + b2\n",
    "            self._predicts = tf.cast(logits > 0, tf.int32)\n",
    "\n",
    "        with tf.variable_scope('Loss'):\n",
    "            labels = tf.cast(self._targets, tf.float32)\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=labels, logits=logits)\n",
    "            self._loss_op = tf.reduce_mean(losses, name='loss')\n",
    "\n",
    "        with tf.variable_scope('BackProp'):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "            trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            grads_and_vars = optimizer.compute_gradients(\n",
    "                loss=self._loss_op, var_list=trainable_vars)\n",
    "            self._train_op = optimizer.apply_gradients(\n",
    "                grads_and_vars=grads_and_vars, \n",
    "                global_step=self._gs,\n",
    "                name=\"train_op\")\n",
    "            \n",
    "        with tf.variable_scope('Accuracy'):\n",
    "            self._accuracy_op = tf.reduce_mean(\n",
    "                tf.cast(tf.equal(self._predicts, self._targets), tf.float32),\n",
    "                name='accuracy')\n",
    "            \n",
    "            \n",
    "    @property\n",
    "    def global_step(self):\n",
    "        return self._gs\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._inputs\n",
    "    \n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "    \n",
    "    @property\n",
    "    def predictions(self):\n",
    "        return self._predicts\n",
    "    \n",
    "    @property\n",
    "    def loss_op(self):\n",
    "        return self._loss_op\n",
    "    \n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "    \n",
    "    @property\n",
    "    def accuracy_op(self):\n",
    "        return self._accuracy_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The naive way: feeding data step by step\n",
    "The first and more straightfowrard way to push data into a computational graph is to feed actual values into placeholders step by step. This is achievable through the `feed_dict` argument of the `tf.Session.run()` method. A `feed_dict` is a dictionary where keys are TF placeholders and values are `numpy` arrays. So let's build our model with placeholders as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_prepare_graph()\n",
    "with tf.variable_scope('Input') as scope:\n",
    "    inputs = tf.placeholder(dtype=tf.float32, shape=[None, 1024])\n",
    "    targets = tf.placeholder(dtype=tf.int32, shape=[None, 1])\n",
    "model = Model(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run out training, generating ranom examples at each training step and feeding them into the graph via the `feed_dict` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 - loss:0.883482 - accuracy:0.507812\n",
      "iter:10 - loss:1.639438 - accuracy:0.503906\n",
      "iter:20 - loss:1.340737 - accuracy:0.527344\n",
      "iter:30 - loss:1.707782 - accuracy:0.472656\n",
      "iter:40 - loss:0.821126 - accuracy:0.617188\n",
      "iter:50 - loss:0.732072 - accuracy:0.667969\n",
      "iter:60 - loss:0.714731 - accuracy:0.671875\n",
      "iter:70 - loss:0.540966 - accuracy:0.722656\n",
      "iter:80 - loss:0.320460 - accuracy:0.843750\n",
      "iter:90 - loss:0.310457 - accuracy:0.855469\n",
      "Time taken: 6.120873\n"
     ]
    }
   ],
   "source": [
    "def get_batch_tensors(batch_size=128):\n",
    "    data = tf.random_normal([batch_size, 1024], mean=0, stddev=1)\n",
    "    labels = tf.cast(tf.reduce_sum(data, axis=1, keep_dims=True) > 0, tf.int32)\n",
    "    return data, labels\n",
    "\n",
    "STEPS = 100\n",
    "CHECK_EVERY = 10\n",
    "\n",
    "startTime = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(STEPS):\n",
    "        actual_inputs, actual_targets = sess.run(list(get_batch_tensors(256)))\n",
    "        fetches = [model.train_op]\n",
    "        if i % CHECK_EVERY == 0:\n",
    "            fetches = fetches + [model.loss_op, model.accuracy_op]\n",
    "        results = sess.run(\n",
    "             fetches=fetches,\n",
    "             feed_dict={\n",
    "                 model.inputs: actual_inputs,\n",
    "                 model.targets: actual_targets\n",
    "             })\n",
    "        if len(results) > 1:\n",
    "            print('iter:%d - loss:%f - accuracy:%f' % (i, results[1], results[2]))\n",
    "print(\"Time taken: %f\" % (time.time() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario:\n",
    "* everything is synchronous and single-threaded\n",
    "* we keep moving back and forth, between Python and the underlying C++ wrapper and between the CPU and the GPU.\n",
    "\n",
    "(*Aside, if you run a `nvidia-smi` while running this example, you would see a low usage of the GPU*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speedig up your training with Queues\n",
    "Queues are a powerful mechanism for asynchronous computation using TF. Like everything in TF, a queue is a node in a graph. It's a stateful node, like a variable: other nodes can modify its content. In particular, nodes can enqueue new items in to the queue, or dequeue existing items from the queue.  \n",
    "  \n",
    "To use a queue in our model, we have to rewrite the input creation. We will:\n",
    "* crete a `FIFOQueue` object\n",
    "* enqueue a pair of batch of tensor (inputs, features) in it\n",
    "* create a `QueueRunner` which will be on duty of running the queue across different threads\n",
    "* add the queue runner to the `QUEUE_RUNNERS` graph collection\n",
    "* dequeue the (inputs, features) batches from the queue\n",
    "* feed such tensors as inputs to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_prepare_graph()\n",
    "\n",
    "NUM_THREADS = 2\n",
    "FIFOQUEUE_NAME = 'FIFOQueue'\n",
    "\n",
    "with tf.variable_scope('Input') as scope:\n",
    "    inputs, targets = get_batch_tensors(128)\n",
    "    queue = tf.FIFOQueue(capacity=5, dtypes=[tf.float32, tf.int32], name=FIFOQUEUE_NAME)\n",
    "    enqueue_op = queue.enqueue((inputs, targets))\n",
    "    queue_runner = tf.train.QueueRunner(queue, [enqueue_op] * NUM_THREADS)\n",
    "    tf.train.add_queue_runner(queue_runner)\n",
    "    inputs, targets = queue.dequeue()\n",
    "model = Model(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instruction `tf.train.add_queue_runner(queue_runner)` adds the queue runner of the queue (with name \"FIFOQueue\") in the `QUEUE_RUNNERS` collection of the graph. We can inspect such collection to verify.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)[0].name == 'Input/' + FIFOQUEUE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a graph with queues, we have to perform some more operation that the usual. We will get through them using the `tf.InteractiveSession` which is just a regular session that can support interactive environments (like in notebooks or IDLE). First, as usual, we have to create the session and run the graph initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session initiaized. The global step is: 0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Session initiaized. The global step is: %d' % sess.run(model.global_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queue runners will start threads that run the input pipeline, filling the example queue so that the dequeue to get the examples will succeed. To coordinate all the runners, we can use a `tf.Coordinator` object to control them. We create a coordinator instance and pass it as an argument to the `tf.train.start_queue_runners` function that will return a list of `Thread` object -- typically in number greater than the selected number of threads for coordination issues.\n",
    "  \n",
    "**NOTA BENE**: the queue runners must be started **before** any train/inference operation that, otherwise, will hang indefinetly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queue runners started, 3 threads created.\n"
     ]
    }
   ],
   "source": [
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(coord=coord)\n",
    "print('Queue runners started, %d threads created.' % len(threads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the regular training (for a smaller amount of times, just to run something)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 - loss:1.423948 - accuracy:0.468750\n"
     ]
    }
   ],
   "source": [
    "STEPS = 5\n",
    "CHECK_EVERY = 5\n",
    "for i in range(STEPS):        \n",
    "    fetches = [model.train_op]\n",
    "    if i % CHECK_EVERY == 0:\n",
    "        fetches = fetches + [model.loss_op, model.accuracy_op]\n",
    "    results = sess.run(fetches)\n",
    "    if len(results) > 1:\n",
    "        print('iter:%d - loss:%f - accuracy:%f' % (i, results[1], results[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the training, we can use the coordinator to stop the queue runners and join the threads.  \n",
    "P.S. don't forget to close the *interactive* session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put everything together and run the same process in a more common way, measuring the performance increment in terms of time spent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 - loss:1.423948 - accuracy:0.468750\n",
      "iter:10 - loss:1.059269 - accuracy:0.546875\n",
      "iter:20 - loss:1.995191 - accuracy:0.570312\n",
      "iter:30 - loss:1.205318 - accuracy:0.460938\n",
      "iter:40 - loss:1.159184 - accuracy:0.507812\n",
      "iter:50 - loss:0.887080 - accuracy:0.609375\n",
      "iter:60 - loss:0.444107 - accuracy:0.804688\n",
      "iter:70 - loss:0.532430 - accuracy:0.718750\n",
      "iter:80 - loss:0.485949 - accuracy:0.789062\n",
      "iter:90 - loss:0.270457 - accuracy:0.859375\n",
      "Time taken: 3.325673\n"
     ]
    }
   ],
   "source": [
    "STEPS = 100\n",
    "CHECK_EVERY = 10\n",
    "\n",
    "startTime = time.time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    for i in range(STEPS):        \n",
    "        fetches = [model.train_op]\n",
    "        if i % CHECK_EVERY == 0:\n",
    "            fetches = fetches + [model.loss_op, model.accuracy_op]\n",
    "        results = sess.run(fetches)\n",
    "        if len(results) > 1:\n",
    "            print('iter:%d - loss:%f - accuracy:%f' % (i, results[1], results[2]))\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "    print(\"Time taken: %f\" % (time.time() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A proper input pipeline\n",
    "To describe how to set up a proper input pipeline, we will simulate a more realistic scenario. Let's assume we are dealing with a large dataset persisted on 20 `.CSV` files, each one with 100 examples. We will start generating it with the following simple snippet.  \n",
    "  \n",
    "Each example is represented in a line with 1025 float numbers, separated by a `,`. The first 1024 elements represents the input feature, while the last one represent the gold truth label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/tmp/TF-102-05/data'\n",
    "FILE_ROOT = 'examples-'\n",
    "FILE_EXT = '.csv'\n",
    "SEP = ','\n",
    "\n",
    "def generate_data():\n",
    "    os.makedirs(DATA_DIR)\n",
    "    features = tf.random_normal([1024], mean=0.0, stddev=1.0)\n",
    "    label = tf.cast(tf.reduce_sum(features) > 0, dtype=tf.float32)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        for i in range(20):\n",
    "            fname = os.path.join(DATA_DIR, FILE_ROOT + str(i).zfill(2) + FILE_EXT)\n",
    "            with open(fname, 'w') as fio:\n",
    "                for j in range(100):\n",
    "                    f, l = sess.run([features, label])\n",
    "                    line = SEP.join([str(item) for item in f] + [str(l)]) + '\\n'\n",
    "                    fio.write(line)\n",
    "            \n",
    "if not os.path.exists(DATA_DIR):\n",
    "    generate_data()\n",
    "    \n",
    "_prepare_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples-00.csv\n",
      "examples-01.csv\n",
      "examples-02.csv\n",
      "examples-03.csv\n",
      "examples-04.csv\n",
      "examples-05.csv\n",
      "\n",
      "examples-15.csv\n",
      "examples-16.csv\n",
      "examples-17.csv\n",
      "examples-18.csv\n",
      "examples-19.csv\n",
      "\n",
      "100 examples-00.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /tmp/TF-102-05/data\n",
    "ls | head -6\n",
    "echo\n",
    "ls | tail -5\n",
    "echo\n",
    "wc -l examples-00.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a queue reading from a set of file names, shuffling them. We can set the number of epochs so that the training stops after the training set has been submittet to the model such number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "PATTERN = DATA_DIR + '/' + FILE_ROOT + '*' + FILE_EXT\n",
    "files = tf.train.match_filenames_once(PATTERN)\n",
    "filename_queue = tf.train.string_input_producer(files, name='FilenameQueue', shuffle=True, num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function already register a proper queue runner in the `QUEUE_RUNNERS` collection.  \n",
    "**NOTA BENE**: this process will create some local variables that **MUST** be initialized when running the session via the `tf.local_variables_initialize()` op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueueRunners:\n",
      " FilenameQueue\n",
      "\n",
      "LocalVariables\n",
      "  matching_filenames\n",
      "  FilenameQueue/limit_epochs/epochs\n"
     ]
    }
   ],
   "source": [
    "print('QueueRunners:')\n",
    "for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "    print(' ' + qr.name)\n",
    "print\n",
    "\n",
    "print('LocalVariables')\n",
    "for var in tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES):\n",
    "    print('  ' + var.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a reader object that is fed with the queue and returns two tensors:\n",
    "* a `key` tensor of dtype `tf.string` that identifies the particular element in the queue;\n",
    "* a `value` tensor that holds the actual value of the example;\n",
    "the `value` tensor must be fed into a decoder function that turns it into a set of tensors that can be fed into the model.  \n",
    "  \n",
    "Since we are dealing with `.CSV` files, we can use off-the-shelf components that are already in the TF library, namely the `tf.TextLineReader` and the `tf.decode_csv` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "columns = tf.decode_csv(value, record_defaults=[[0.0]] * 1025)\n",
    "inputs, targets = tf.stack(columns[:-1]), tf.cast(tf.expand_dims(input=columns[-1], dim=-1), tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's have a brief recap:\n",
    "1. a set of filename is enqueued and fed into a reader;\n",
    "2. the reader reads (a key and) a value, i.e. a tensor representing a single example;\n",
    "3. such tensor is fed into a decoder that returns some other tensors (depending on the parsing).\n",
    "  \n",
    "Keep in mind: `filename queue -> reader -> value into the decoder.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can build another queue to read pairs of `inputs` and `targets`, batch and shuffle them. We can use the `tf.train.shuffle_batch` function accepting some interesting parameters:\n",
    "* `capacity`: An integer. The maximum number of elements in the queue.\n",
    "* `min_after_dequeue`: Minimum number elements in the queue after a dequeue, used to ensure a level of mixing of elements.\n",
    "* `num_threads`: The number of threads enqueuing the input tensors.\n",
    "* `allow_smaller_final_batch`: (Optional) Boolean. If True, allow the final batch to be smaller if there are insufficient items left in the queue.  \n",
    "  \n",
    "The value of `capacity` must be larger than `min_after_dequeue` and the amount larger determines the maximum that will be prefetch. A rule of thumb is that:  \n",
    "  \n",
    "    min_after_dequeue + (num_threads + a_small_safety_margin) * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 23\n",
    "MIN_AFTER_DEQUEUE = 10\n",
    "NUM_THREADS = 2\n",
    "CAPACITY = MIN_AFTER_DEQUEUE + (NUM_THREADS + 1) * BATCH_SIZE\n",
    "\n",
    "input_batch, target_batch = tf.train.shuffle_batch(\n",
    "    tensors=[inputs, targets],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_threads=NUM_THREADS,\n",
    "    capacity=CAPACITY,\n",
    "    min_after_dequeue=MIN_AFTER_DEQUEUE,\n",
    "    allow_smaller_final_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the `tf.train.shuffle_batch` will create some other queue runners for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueueRunners:\n",
      " FilenameQueue\n",
      " shuffle_batch/random_shuffle_queue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('QueueRunners:')\n",
    "for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "    print(' ' + qr.name)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build and train the model. Since we are iterating over the examples for a certain number of epochs, the queue will raise a `tf.errors.OutOfRangeError` that can be caught and trigger the end of the training process. Again, the `tf.train.Coordinator` class has everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step: 20\n",
      "global_step: 40\n",
      "global_step: 60\n",
      "global_step: 80\n",
      "global_step: 100\n",
      "global_step: 120\n",
      "global_step: 140\n",
      "global_step: 160\n",
      "Done training, epoch limit reached.\n",
      "last global step: 174\n",
      "last batch of size: 21\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_batch, target_batch)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.local_variables_initializer())  # again: keep in mind!\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            train_op, global_step, predictions = sess.run(\n",
    "                [model.train_op, model.global_step, model.predictions])\n",
    "            if global_step % 20 == 0:\n",
    "                print('global_step: %d' % global_step)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training, epoch limit reached.')\n",
    "        print('last global step: %d' % global_step)\n",
    "        print('last batch of size: %d' % len(predictions))\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
