{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See what's going on with `tf.Summary` and the `TensorBoard`\n",
    "\n",
    "When you are training your model, you want want to keep track of how things are going, how the weights of your model are evolving, if gradients are exploding or vanishing (or, hopefully, none of these), which is the accuracy of your model, ecc.  \n",
    "  \n",
    "The naive way to do this would be to build a custom way to store, read and visualize such data but `TensorFlow` has a better solution: using `tf.Summary`, you can easily save them (as `protobufs`) and using the `TensorBoard` you can read and visualize them, together with the graph structure and many other features.  \n",
    "  \n",
    "First of all, of course..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Task\n",
    "For this tutorial, we need a reference task. It is a super simple task so that we can focus on *how* we are doing things without paying too much attention to *what* we are doing. The task is the following: \n",
    "  \n",
    "> given a vector of N=1024 components, label it with `1` if the sum of its component is `>=0`, otherwise label it with `0`.  \n",
    "  \n",
    "The model we are implementing is a two-layers MLP, where the first layer is [1024, 1024] with ReLU and the second is [1024, 1] with a sigmoid. We use cross-entropy as loss function and the Adam learning algorithm.  \n",
    "  \n",
    "First of all, we create a `global_step` variable, not trainable, which is used to keep the value of the global step of the training process. This is a common practice and it can be stored/accessed through the `tf.GraphKeys.GLOBAL_STEP` key. Then, we create the placeholders for the input data and the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int32)\n",
    "\n",
    "with tf.variable_scope('Placeholder'):\n",
    "    input = tf.placeholder(dtype=tf.float32, shape=[None, 1024])\n",
    "    labels = tf.placeholder(dtype=tf.int32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the two hidden layers of the network, we build summaries for all the weights and biases, logits and activarions. We use the function `summarize()` that accepts as input a `Tensor` and attaches some summaries to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize(var, scope='summaries'):\n",
    "    with tf.name_scope(scope):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the first layer `Layer1` and add invoke the `summarize` function for all the tensors that have been created, one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Layer1') as scope:\n",
    "    w1 = tf.get_variable('w', shape=[1024, 1024])\n",
    "    b1 = tf.get_variable('b', shape=[1024])\n",
    "    z1 = tf.add(tf.matmul(input, w1), b1, name='potential')\n",
    "    y1 = tf.nn.relu(z1, name='activation')\n",
    "    \n",
    "    summarize(w1, w1.op.name)\n",
    "    summarize(b1, b1.op.name)\n",
    "    summarize(z1, z1.op.name)\n",
    "    summarize(y1, y1.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that what is going on is clearer, let's collect and add the variables in a smarter way, using the name scope and the `GLOBAL_VARIABLES` collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Layer2') as scope:\n",
    "    w2 = tf.get_variable('w', shape=[1024, 1])\n",
    "    b2 = tf.get_variable('b', shape=[1])\n",
    "    logits = tf.add(tf.matmul(y1, w2), b2, name='logits')\n",
    "    predicts = tf.cast(logits > 0, tf.int32)\n",
    "    \n",
    "    scope_vars = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) if v.name.startswith(scope.name)]\n",
    "    for var in scope_vars:\n",
    "        summarize(var, var.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will track also the summaries for the loss and for the gradients -- but since the loss is a scalar value, we don't need to use the `summarize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(labels, tf.float32), logits=logits)\n",
    "    loss_op = tf.reduce_mean(losses, name='loss')\n",
    "    loss_summary_op = tf.summary.scalar(tensor=loss_op, name='loss_value')\n",
    "\n",
    "with tf.variable_scope('BackProp'):\n",
    "    adam = tf.train.GradientDescentOptimizer(0.1)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    grads_and_vars = adam.compute_gradients(loss=loss_op, var_list=trainable_vars)\n",
    "    train_op = adam.apply_gradients(grads_and_vars=grads_and_vars, name=\"train_op\")\n",
    "    for grad, _ in grads_and_vars:\n",
    "        summarize(grad, grad.op.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the TF summaries to print evaluation metrics. In our case, we will use also attach a scalar summary to the accuracy that we measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Accuracy'):\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(tf.equal(predicts, labels), tf.float32), name='accuracy')\n",
    "    accuracy_summary_op = tf.summary.scalar(tensor=accuracy_op, name='accuracy_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going to the actual training, let's inspect the `tf.GraphKeys.SUMMARIES` collection and check what's in there. We will find all the summaries we have been creating so far that are automatically added to the proper collection. Finally, we can merge all of them into a single op that will be computed at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer1/Layer1/w/mean:0\n",
      "Layer1/Layer1/w/stddev_1:0\n",
      "Layer1/Layer1/w/max:0\n",
      "Layer1/Layer1/w/min:0\n",
      "Layer1/Layer1/w/histogram:0\n",
      "Layer1/Layer1/b/mean:0\n",
      "Layer1/Layer1/b/stddev_1:0\n",
      "Layer1/Layer1/b/max:0\n",
      "Layer1/Layer1/b/min:0\n",
      "Layer1/Layer1/b/histogram:0\n",
      "Layer1/Layer1/potential/mean:0\n",
      "Layer1/Layer1/potential/stddev_1:0\n",
      "Layer1/Layer1/potential/max:0\n",
      "Layer1/Layer1/potential/min:0\n",
      "Layer1/Layer1/potential/histogram:0\n",
      "Layer1/Layer1/activation/mean:0\n",
      "Layer1/Layer1/activation/stddev_1:0\n",
      "Layer1/Layer1/activation/max:0\n",
      "Layer1/Layer1/activation/min:0\n",
      "Layer1/Layer1/activation/histogram:0\n",
      "Layer2/Layer2/w/mean:0\n",
      "Layer2/Layer2/w/stddev_1:0\n",
      "Layer2/Layer2/w/max:0\n",
      "Layer2/Layer2/w/min:0\n",
      "Layer2/Layer2/w/histogram:0\n",
      "Layer2/Layer2/b/mean:0\n",
      "Layer2/Layer2/b/stddev_1:0\n",
      "Layer2/Layer2/b/max:0\n",
      "Layer2/Layer2/b/min:0\n",
      "Layer2/Layer2/b/histogram:0\n",
      "Loss/loss_value:0\n",
      "BackProp/BackProp/gradients/Layer1/MatMul_grad/tuple/control_dependency_1/mean:0\n",
      "BackProp/BackProp/gradients/Layer1/MatMul_grad/tuple/control_dependency_1/stddev_1:0\n",
      "BackProp/BackProp/gradients/Layer1/MatMul_grad/tuple/control_dependency_1/max:0\n",
      "BackProp/BackProp/gradients/Layer1/MatMul_grad/tuple/control_dependency_1/min:0\n",
      "BackProp/BackProp/gradients/Layer1/MatMul_grad/tuple/control_dependency_1/histogram:0\n",
      "BackProp/BackProp/gradients/Layer1/potential_grad/tuple/control_dependency_1/mean:0\n",
      "BackProp/BackProp/gradients/Layer1/potential_grad/tuple/control_dependency_1/stddev_1:0\n",
      "BackProp/BackProp/gradients/Layer1/potential_grad/tuple/control_dependency_1/max:0\n",
      "BackProp/BackProp/gradients/Layer1/potential_grad/tuple/control_dependency_1/min:0\n",
      "BackProp/BackProp/gradients/Layer1/potential_grad/tuple/control_dependency_1/histogram:0\n",
      "BackProp/BackProp/gradients/Layer2/MatMul_grad/tuple/control_dependency_1/mean:0\n",
      "BackProp/BackProp/gradients/Layer2/MatMul_grad/tuple/control_dependency_1/stddev_1:0\n",
      "BackProp/BackProp/gradients/Layer2/MatMul_grad/tuple/control_dependency_1/max:0\n",
      "BackProp/BackProp/gradients/Layer2/MatMul_grad/tuple/control_dependency_1/min:0\n",
      "BackProp/BackProp/gradients/Layer2/MatMul_grad/tuple/control_dependency_1/histogram:0\n",
      "BackProp/BackProp/gradients/Layer2/logits_grad/tuple/control_dependency_1/mean:0\n",
      "BackProp/BackProp/gradients/Layer2/logits_grad/tuple/control_dependency_1/stddev_1:0\n",
      "BackProp/BackProp/gradients/Layer2/logits_grad/tuple/control_dependency_1/max:0\n",
      "BackProp/BackProp/gradients/Layer2/logits_grad/tuple/control_dependency_1/min:0\n",
      "BackProp/BackProp/gradients/Layer2/logits_grad/tuple/control_dependency_1/histogram:0\n",
      "Accuracy/accuracy_value:0\n",
      "\n",
      "Tensor(\"add:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for var in tf.get_collection(tf.GraphKeys.SUMMARIES):\n",
    "    print var.name\n",
    "print\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "print('Summary Op: ' + summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function `get_batch_tensors(batch_size=128)` that generates a batch of input data and output target labels og a given `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_tensors(batch_size=128):\n",
    "    data = tf.random_normal([batch_size, 1024], mean=0, stddev=1)  # why 5000? It's a lot!\n",
    "    labels = tf.cast(tf.reduce_sum(data, axis=1, keep_dims=True) > 0, tf.int32)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the Graph structure and the summaries we attached to our variables, we have to create a `tf.summary.FileWriter` with a directory and a `tf.Graph` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "LOGDIR = '/tmp/TF-102-04'\n",
    "if os.path.isdir(LOGDIR):\n",
    "    shutil.rmtree(LOGDIR)\n",
    "os.mkdir(LOGDIR)\n",
    "\n",
    "writer = tf.summary.FileWriter(logdir=LOGDIR, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training, when we compute the summary op, we just need to add it to the `writer` indicating the global step, so that we can keep track of the evolution of the varlues we are monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss: 0.812866, accuracy: 0.476562\n",
      "step 10: loss: 0.809555, accuracy: 0.445312\n",
      "step 20: loss: 0.819426, accuracy: 0.515625\n",
      "step 30: loss: 0.787941, accuracy: 0.515625\n",
      "step 40: loss: 0.784940, accuracy: 0.531250\n",
      "step 50: loss: 0.742575, accuracy: 0.492188\n",
      "step 60: loss: 0.823784, accuracy: 0.453125\n",
      "step 70: loss: 0.808247, accuracy: 0.507812\n",
      "step 80: loss: 0.854534, accuracy: 0.460938\n",
      "step 90: loss: 0.752069, accuracy: 0.507812\n"
     ]
    }
   ],
   "source": [
    "STEPS = 100\n",
    "EVERY_STEPS = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(STEPS):\n",
    "        actual_input_tensor, actual_label_tensor = get_batch_tensors()\n",
    "        actual_input, actual_labels = sess.run([actual_input_tensor, actual_label_tensor])\n",
    "        \n",
    "        feed_dict = {\n",
    "            input: actual_input,\n",
    "            labels: actual_labels\n",
    "        }\n",
    "        \n",
    "        summary, loss, accuracy = sess.run(\n",
    "            fetches=[summary_op, loss_op, accuracy_op],\n",
    "            feed_dict=feed_dict)\n",
    "        writer.add_summary(summary=summary, global_step=i)\n",
    "        if i % EVERY_STEPS == 0:\n",
    "            print('step %d: loss: %f, accuracy: %f' % (i, loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the `TensorBoard` on the log directory we are writing to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m tensorflow.tensorboard --logdir=/tmp/TF-102-04"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
