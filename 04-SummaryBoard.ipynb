{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See what's going on with `tf.Summary` and the `TensorBoard`\n",
    "\n",
    "When you are training your model, you want want to keep track of how things are going, how the weights of your model are evolving, if gradients are exploding or vanishing (or, hopefully, none of these), which is the accuracy of your model, ecc.  \n",
    "  \n",
    "The naive way to do this would be to build a custom way to store, read and visualize such data but `TensorFlow` has a better solution: using `tf.Summary`, you can easily save them (as `protobufs`) and using the `TensorBoard` you can read and visualize them, together with the graph structure and many other features.  \n",
    "  \n",
    "First of all, of course..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Task\n",
    "For this tutorial, we need a reference task. It is a super simple task so that we can focus on *how* we are doing things without paying too much attention to *what* we are doing. The task is the following: \n",
    "  \n",
    "> given a vector of N=1024 components, label it with `1` if the sum of its component is `>=0`, otherwise label it with `0`.  \n",
    "  \n",
    "The model we are implementing is a two-layers MLP, where the first layer is [1024, 1024] with ReLU and the second is [1024, 1] with a sigmoid. We use cross-entropy as loss function and the Adam learning algorithm.  \n",
    "  \n",
    "First of all, we create a `global_step` variable, not trainable, which is used to keep the value of the global step of the training process. This is a common practice and it can be stored/accessed through the `tf.GraphKeys.GLOBAL_STEP` key. Then, we create the placeholders for the input data and the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs = tf.Variable(0, name='global_step', trainable=False, dtype=tf.int32)\n",
    "\n",
    "with tf.variable_scope('Placeholder'):\n",
    "    input = tf.placeholder(dtype=tf.float32, shape=[None, 1024])\n",
    "    labels = tf.placeholder(dtype=tf.int32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the two hidden layers of the network, we build summaries for all the weights and biases, logits and activarions. We use the function `summarize()` that accepts as input a `Tensor` and attaches some summaries to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize(var, scope='summaries'):\n",
    "    with tf.name_scope(scope):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the first layer `Layer1` and add invoke the `summarize` function for all the tensors that have been created, one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Layer1') as scope:\n",
    "    w1 = tf.get_variable('w', shape=[1024, 1024])\n",
    "    b1 = tf.get_variable('b', shape=[1024])\n",
    "    z1 = tf.matmul(input, w1) + b1\n",
    "    y1 = tf.nn.relu(z1, name='activation')\n",
    "    \n",
    "    summarize(w1, w1.op.name)\n",
    "    summarize(b1, b1.op.name)\n",
    "    summarize(z1, z1.op.name)\n",
    "    summarize(y1, y1.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that what is going on is clearer, let's collect and add the variables in a smarter way, using the name scope and the `GLOBAL_VARIABLES` collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Layer2') as scope:\n",
    "    w2 = tf.get_variable('w', shape=[1024, 1])\n",
    "    b2 = tf.get_variable('b', shape=[1])\n",
    "    logits = tf.matmul(y1, w2) + b2\n",
    "    predicts = tf.cast(logits > 0, tf.int32)\n",
    "    \n",
    "    scope_vars = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) if v.name.startswith(scope.name)]\n",
    "    for var in scope_vars:\n",
    "        summarize(var, var.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will track also the summaries for the loss and for the gradients -- but since the loss is a scalar value, we don't need to use the `summarize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Loss'):\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.cast(labels, tf.float32), logits=logits)\n",
    "    loss_op = tf.reduce_mean(losses, name='loss')\n",
    "    loss_summary_op = tf.summary.scalar(tensor=loss_op, name='loss_value')\n",
    "\n",
    "with tf.variable_scope('BackProp'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss=loss_op, var_list=trainable_vars)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars=grads_and_vars, name=\"train_op\")\n",
    "    for grad, _ in grads_and_vars:\n",
    "        summarize(grad, grad.op.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the TF summaries to print evaluation metrics. In our case, we will use also attach a scalar summary to the accuracy that we measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Accuracy'):\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(tf.equal(predicts, labels), tf.float32), name='accuracy')\n",
    "    accuracy_summary_op = tf.summary.scalar(tensor=accuracy_op, name='accuracy_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going to the actual training, let's inspect the `tf.GraphKeys.SUMMARIES` collection and check what's in there. We will find all the summaries we have been creating so far that are automatically added to the proper collection. Finally, we can merge all of them into a single op that will be computed at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer1/Layer1/w/mean:0\n",
      "Layer1/Layer1/w/stddev_1:0\n",
      "Layer1/Layer1/w/max:0\n",
      "Layer1/Layer1/w/min:0\n",
      "Layer1/Layer1/w/histogram:0\n",
      "Layer1/Layer1/b/mean:0\n",
      "Layer1/Layer1/b/stddev_1:0\n",
      "Layer1/Layer1/b/max:0\n",
      "Layer1/Layer1/b/min:0\n",
      "Layer1/Layer1/b/histogram:0\n",
      "Layer1/Layer1/add/mean:0\n",
      "Layer1/Layer1/add/stddev_1:0\n",
      "Layer1/Layer1/add/max:0\n",
      "Layer1/Layer1/add/min:0\n",
      "Layer1/Layer1/add/histogram:0\n",
      "Layer1/Layer1/activation/mean:0\n",
      "Layer1/Layer1/activation/stddev_1:0\n",
      "Layer1/Layer1/activation/max:0\n",
      "Layer1/Layer1/activation/min:0\n",
      "Layer1/Layer1/activation/histogram:0\n",
      "Layer2/Layer2/w/mean:0\n",
      "Layer2/Layer2/w/stddev_1:0\n",
      "Layer2/Layer2/w/max:0\n",
      "Layer2/Layer2/w/min:0\n",
      "Layer2/Layer2/w/histogram:0\n",
      "Layer2/Layer2/b/mean:0\n",
      "Layer2/Layer2/b/stddev_1:0\n",
      "Layer2/Layer2/b/max:0\n",
      "Layer2/Layer2/b/min:0\n",
      "Layer2/Layer2/b/histogram:0\n",
      "Loss/loss_value:0\n",
      "BackProp/BackProp/gradients/Layer1/MatMul_grad/tuple/control_dependency_1/mean:0\n",
      "BackProp/BackProp/gradients/Layer1/MatMul_grad/tuple/control_dependency_1/stddev_1:0\n",
      "BackProp/BackProp/gradients/Layer1/MatMul_grad/tuple/control_dependency_1/max:0\n",
      "BackProp/BackProp/gradients/Layer1/MatMul_grad/tuple/control_dependency_1/min:0\n",
      "BackProp/BackProp/gradients/Layer1/MatMul_grad/tuple/control_dependency_1/histogram:0\n",
      "BackProp/BackProp/gradients/Layer1/add_grad/tuple/control_dependency_1/mean:0\n",
      "BackProp/BackProp/gradients/Layer1/add_grad/tuple/control_dependency_1/stddev_1:0\n",
      "BackProp/BackProp/gradients/Layer1/add_grad/tuple/control_dependency_1/max:0\n",
      "BackProp/BackProp/gradients/Layer1/add_grad/tuple/control_dependency_1/min:0\n",
      "BackProp/BackProp/gradients/Layer1/add_grad/tuple/control_dependency_1/histogram:0\n",
      "BackProp/BackProp/gradients/Layer2/MatMul_grad/tuple/control_dependency_1/mean:0\n",
      "BackProp/BackProp/gradients/Layer2/MatMul_grad/tuple/control_dependency_1/stddev_1:0\n",
      "BackProp/BackProp/gradients/Layer2/MatMul_grad/tuple/control_dependency_1/max:0\n",
      "BackProp/BackProp/gradients/Layer2/MatMul_grad/tuple/control_dependency_1/min:0\n",
      "BackProp/BackProp/gradients/Layer2/MatMul_grad/tuple/control_dependency_1/histogram:0\n",
      "BackProp/BackProp/gradients/Layer2/add_grad/tuple/control_dependency_1/mean:0\n",
      "BackProp/BackProp/gradients/Layer2/add_grad/tuple/control_dependency_1/stddev_1:0\n",
      "BackProp/BackProp/gradients/Layer2/add_grad/tuple/control_dependency_1/max:0\n",
      "BackProp/BackProp/gradients/Layer2/add_grad/tuple/control_dependency_1/min:0\n",
      "BackProp/BackProp/gradients/Layer2/add_grad/tuple/control_dependency_1/histogram:0\n",
      "Accuracy/accuracy_value:0\n",
      "\n",
      "Tensor(\"add:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for var in tf.get_collection(tf.GraphKeys.SUMMARIES):\n",
    "    print var.name\n",
    "print\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "print('Summary Op: ' + summary_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function `get_batch_tensors(batch_size=128)` that generates a batch of input data and output target labels og a given `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_tensors(batch_size=128):\n",
    "    data = tf.random_normal([batch_size, 1024], mean=0, stddev=1)\n",
    "    labels = tf.cast(tf.reduce_sum(data, axis=1, keep_dims=True) > 0, tf.int32)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the Graph structure and the summaries we attached to our variables, we have to create a `tf.summary.FileWriter` with a directory and a `tf.Graph` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "LOGDIR = '/tmp/TF-102-04'\n",
    "if os.path.isdir(LOGDIR):\n",
    "    shutil.rmtree(LOGDIR)\n",
    "os.mkdir(LOGDIR)\n",
    "\n",
    "writer = tf.summary.FileWriter(logdir=LOGDIR, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training, when we compute the summary op, we just need to add it to the `writer` indicating the global step, so that we can keep track of the evolution of the varlues we are monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss: 0.812866, accuracy: 0.476562\n",
      "step 20: loss: 1.201750, accuracy: 0.476562\n",
      "step 40: loss: 0.799970, accuracy: 0.664062\n",
      "step 60: loss: 0.835575, accuracy: 0.640625\n",
      "step 80: loss: 0.335251, accuracy: 0.843750\n",
      "step 100: loss: 0.298447, accuracy: 0.859375\n",
      "step 120: loss: 0.228983, accuracy: 0.906250\n",
      "step 140: loss: 0.241459, accuracy: 0.898438\n",
      "step 160: loss: 0.239199, accuracy: 0.890625\n",
      "step 180: loss: 0.202862, accuracy: 0.953125\n",
      "step 200: loss: 0.222343, accuracy: 0.914062\n",
      "step 220: loss: 0.145591, accuracy: 0.968750\n",
      "step 240: loss: 0.216175, accuracy: 0.914062\n",
      "step 260: loss: 0.224608, accuracy: 0.906250\n",
      "step 280: loss: 0.161540, accuracy: 0.960938\n",
      "step 300: loss: 0.198756, accuracy: 0.898438\n",
      "step 320: loss: 0.207698, accuracy: 0.890625\n",
      "step 340: loss: 0.150906, accuracy: 0.937500\n",
      "step 360: loss: 0.172924, accuracy: 0.914062\n",
      "step 380: loss: 0.178695, accuracy: 0.929688\n",
      "step 400: loss: 0.199341, accuracy: 0.906250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e86ae3341752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         _, summary, loss, accuracy = sess.run(\n\u001b[1;32m     17\u001b[0m             \u001b[0mfetches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEVERY_STEPS\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "STEPS = 400\n",
    "EVERY_STEPS = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(STEPS):\n",
    "        actual_input_tensor, actual_label_tensor = get_batch_tensors()\n",
    "        actual_input, actual_labels = sess.run([actual_input_tensor, actual_label_tensor])\n",
    "        \n",
    "        feed_dict = {\n",
    "            input: actual_input,\n",
    "            labels: actual_labels\n",
    "        }\n",
    "        \n",
    "        _, summary, loss, accuracy = sess.run(\n",
    "            fetches=[train_op, summary_op, loss_op, accuracy_op],\n",
    "            feed_dict=feed_dict)\n",
    "        writer.add_summary(summary=summary, global_step=i)\n",
    "        if i % EVERY_STEPS == 0:\n",
    "            print('step %d: loss: %f, accuracy: %f' % (i, loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the `TensorBoard` on the log directory we are writing to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m tensorflow.tensorboard --logdir=/tmp/TF-102-04"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
