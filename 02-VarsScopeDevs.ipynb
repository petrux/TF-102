{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables, Scopes and Devices\n",
    "\n",
    "When you build your model, it is a good practice -- when not a true necessity -- to organize your variables in a reasonable way, both *logically* and *physically*. Before getting into the tutorial...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give a variable a proper name and share them\n",
    "A variable scope defines a variable namespace. It is easy to see how a name scope is prepended to the fully qualified name of a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope/x:0\n",
      "Scope/x\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('Scope') as scope:\n",
    "    x = tf.Variable(23, name='x')\n",
    "print(x.name)\n",
    "print(x.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `tf.Variable.name` and the `tf.Variable.op.name` are different. As explained [here](http://stackoverflow.com/a/34729874/1861627), the `tf.Variable.name` property maps to the name of the mutable Tensor in which that variable is stored. *Tensor names are generated from the name of the operation that produces them and the index of the output to which that tensor corresponds* -- which is a variable operation in our case. To With a more *sophisticated* example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope/my_sum_op:0\n",
      "Scope/my_sum_op\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('Scope') as scope:\n",
    "    a = tf.Variable(0, 'a')\n",
    "    b = tf.Variable(0, 'b')\n",
    "    c = tf.add(a, b, name='my_sum_op')\n",
    "print(c.name)\n",
    "print(c.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if straightforward, invoking directly the `tf.Variable` constructor to create a new variable **is not the more convenient way** to create your variables. The constructor will create a new variable each time and this is not what you want when you need to have shared variables. The proper way to overcome this issue is to use the `tf.get_variable()` function and properly set the scope `reuse` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('ReuseScope') as scope:\n",
    "    x1 = tf.get_variable(name='x', dtype=tf.int32, initializer=23)\n",
    "    scope.reuse_variables()  # here!\n",
    "    x2 = tf.get_variable(name='x', dtype=tf.int32, initializer=23)\n",
    "    assert x1 == x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are in a recurrent scenario, when an input vector is fed into some layer: you want to *share the weights* of such layer across the different timesteps. This is achievable through such mechanism, as in the example below. Our `input` has 3 timesteps and each input value is a vector of 4 dimensions. We feed such sequence into a linear layer obtaining 3 outputs of 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variables in our model are:\n",
      "   RecurrentScope/weights\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "inputs = [\n",
    "    tf.random_normal([1, 4]),\n",
    "    tf.random_normal([1, 4]),\n",
    "    tf.random_normal([1, 4]),\n",
    "]\n",
    "outputs = []\n",
    "\n",
    "for t, inp in enumerate(inputs):\n",
    "    with tf.variable_scope('RecurrentScope') as scope:\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()\n",
    "        weights = tf.get_variable(name='weights', shape=[4, 2], dtype=tf.float32)\n",
    "        output = tf.matmul(inp, weights)\n",
    "        outputs.append(output)\n",
    "\n",
    "print('The variables in our model are:')\n",
    "for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n",
    "    print('   ' + var.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTA BENE**: the example above is not the best way (if not the worst! :-)) to implement recurrencies and to represent inputs. Please, focus on what is inside the `for` loop and on how we can reuse already defined variables in order to share them across different operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we use the constructor? We will have *a new variable* created at each time which is *noT* what we expect if we want to reuse/share them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variables in our model are:\n",
      "   RecurrentScope/x\n",
      "   RecurrentScope_1/x\n",
      "   RecurrentScope_2/x\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "for t in range(3):\n",
    "    with tf.variable_scope('RecurrentScope') as scope:\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()\n",
    "        x = tf.Variable(23, name='x', dtype=tf.float32)\n",
    "\n",
    "print('The variables in our model are:')\n",
    "for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n",
    "    print('   ' + var.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place the variable on the proper device\n",
    "We can define a context also for the device we want the variable to be *physically* deploied on. The device name follow a certain convention which is extremely handy in a distributed training scenario. To the extent of this tutorial, it is enough to learn that you can address a device using its type, namely `CPU` or `GPU` and its index--an integer starting from 0 and ranging up to the amount of such devices (minus one, of course :-))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUScope/x has been manually assigned to the device /device:GPU:999\n",
      "CPUScope/y has been manually assigned to the device /device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('GPUScope') as scope:\n",
    "    with tf.device('GPU:999'):\n",
    "        x = tf.constant(23, name='x', dtype=tf.float32)\n",
    "        print('%s has been manually assigned to the device %s' % (x.op.name, x.device))\n",
    "\n",
    "with tf.variable_scope('CPUScope') as scope:\n",
    "    with tf.device('CPU:0'):\n",
    "        y = tf.constant(9, name='y', dtype=tf.float32)\n",
    "        print('%s has been manually assigned to the device %s' % (y.op.name, y.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At run-time[1], if the desired devices are note physically present, TF will raise an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot assign a device to node 'GPUScope/x': Could not satisfy explicit device specification '/device:GPU:999' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n",
      "\t [[Node: GPUScope/x = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 23>, _device=\"/device:GPU:999\"]()]]\n",
      "\n",
      "Caused by op u'GPUScope/x', defined at:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 405, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 883, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 260, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 212, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 370, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n",
      "    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-d20ee0f54f4c>\", line 4, in <module>\n",
      "    x = tf.constant(23, name='x', dtype=tf.float32)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 106, in constant\n",
      "    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "InvalidArgumentError (see above for traceback): Cannot assign a device to node 'GPUScope/x': Could not satisfy explicit device specification '/device:GPU:999' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n",
      "\t [[Node: GPUScope/x = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 23>, _device=\"/device:GPU:999\"]()]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "except Exception as e:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ask it to softly place your variable and it will search for a fallback strategy. Typically, TF will try to allocate variables on `GPU:0`, falling back on the `CPU` if a `GPU` is not detected on the machine. If there is only one `GPU:0` available on the machine and you explicitly use a `GPU:1`, TF will fall back on the available devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "try:\n",
    "    with tf.Session(config=tf.ConfigProto(\n",
    "            log_device_placement=True,\n",
    "            allow_soft_placement=True)) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run(x))\n",
    "        print(sess.run(y))\n",
    "except Exception as e:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Jupyter Notebooks don't get along with the TF logging system, but in the console you are running the kernel in, you should see the following:\n",
    "\n",
    "```bash\n",
    "2017-05-01 00:31:51.376794: I tensorflow/core/common_runtime/direct_session.cc:257] Device mapping:\n",
    "\n",
    "init_10: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378591: I tensorflow/core/common_runtime/simple_placer.cc:841] init_10: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
    "init_9: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378649: I tensorflow/core/common_runtime/simple_placer.cc:841] init_9: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
    "init_8: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378683: I tensorflow/core/common_runtime/simple_placer.cc:841] init_8: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
    "init_7: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378715: I tensorflow/core/common_runtime/simple_placer.cc:841] init_7: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
    "init_6: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378747: I tensorflow/core/common_runtime/simple_placer.cc:841] init_6: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
    "init_5: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378779: I tensorflow/core/common_runtime/simple_placer.cc:841] init_5: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
    "init_4: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378810: I tensorflow/core/common_runtime/simple_placer.cc:841] init_4: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
    "init_3: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378841: I tensorflow/core/common_runtime/simple_placer.cc:841] init_3: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
    "init_2: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378872: I tensorflow/core/common_runtime/simple_placer.cc:841] init_2: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
    "init_1: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378927: I tensorflow/core/common_runtime/simple_placer.cc:841] init_1: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
    "init: (NoOp): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378954: I tensorflow/core/common_runtime/simple_placer.cc:841] init: (NoOp)/job:localhost/replica:0/task:0/cpu:0\n",
    "CPUScope/y: (Const): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.378985: I tensorflow/core/common_runtime/simple_placer.cc:841] CPUScope/y: (Const)/job:localhost/replica:0/task:0/cpu:0\n",
    "GPUScope/x: (Const): /job:localhost/replica:0/task:0/cpu:0\n",
    "2017-05-01 00:31:51.379016: I tensorflow/core/common_runtime/simple_placer.cc:841] GPUScope/x: (Const)/job:localhost/replica:0/task:0/cpu:0\n",
    "^[[B^[[B[I 00:32:12.488 NotebookApp] Saving file at /02-VarsScopeDevs.ipynb\n",
    "\n",
    "Device mapping: no known devices.\n",
    "```\n",
    "\n",
    "where we can see how `x` and `y` have been placed to some available devices:\n",
    "```bash\n",
    "...\n",
    "CPUScope/y: (Const): /job:localhost/replica:0/task:0/cpu:0\n",
    "GPUScope/x: (Const): /job:localhost/replica:0/task:0/cpu:0\n",
    "...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
